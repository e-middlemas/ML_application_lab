{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning with Meteorological Station Data: Part 2\n",
    "__Constructed and written by Eleanor Middlemas  \n",
    "August 2020  \n",
    "*elmiddlemas at gmail.com*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2) __Supervised Learning: Relationship assessment & feature importance.__ <br>\n",
    "Which variable in the Christman data is the best predictor of rainfall? <br><br>\n",
    "[Part A) Building a model to predict rainfall likelihood](#Building-a-model-to-predict-rainfall-likelihood) <br>Create a model that best captures the relationship between meteorological variables and rainfall likelihood. <br><br>\n",
    "[Part B) Using our new model to determine best rainfall predictor](#Feature-Importance) <br> Learn how to utilize scikit-learn's *feature importance* tool as an alternative to linear regression.<br><br>\n",
    "[Part C) Post-lab Questions](#Questions)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The goals of Part 2 of this Application Lab are to:__<br>\n",
    "1. Be exposed to a swath of supervised learning algorithms and have some ML algorithm code as a reference for later use.<br>\n",
    "2. Walk away with some understanding of two uses of supervised learning algorithms: prediction and feature importance.<br>\n",
    "3. Become familiar with the data processing pipeline required to utilize supervised learning techniques.<br>\n",
    "4. Learn more vocabulary and building a foundation for future google searches: cross-validation, training v. testing data, metrics (accuracy, recall, precision, f1 score, etc), overfitting/underfitting, balancing datasets, hyperparameters, & feature importance.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We are using the 2016 Christman dataset once again.__ In fact, much like Part 1 of the application lab, we are building and training models to predict something we already know from the dataset--in this case, if it's raining or not. The point of this application lab is not to conduct cutting-edge research or make novel predictions. Instead, __the lab's purpose is to be an overview of ML methods__. The goal is that you can walk away with more confidence to learn ML tools in greater detail your own, if you choose to apply them to those more exciting research questions in your own research.<br><br>\n",
    "*__Note__: Because we randomize our selection of data that we use to train the models, you may find slightly different model metrics, and thus, different answers than your neighbor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0. Read in data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is the same code used in Part 1 of the Application Lab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./christman_2016.csv\")\n",
    "# preview data (also through df.head() & df.tail())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many days are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.day.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be using supervised learning methods to perform the following two tasks:\n",
    "1. Predict the likelihood of rainfall given certain atmospheric conditions\n",
    "    1. Prepping the data\n",
    "    2. Building and training models\n",
    "        1. Logistic regression\n",
    "        2. Random Forest\n",
    "        3. Singular vector machines/classifier\n",
    "        4. Neural Network\n",
    "2. Determine which variable (\"feature\") is the best predictor of rainfall, i.e., \"feature importance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Building a model to predict rainfall likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to determine which features or atmospheric variables are the best predictors of rainfall in Christman, CO. <br><br>\n",
    "Often, one simply regresses some metric of precipitation onto various atmospheric variables, and assume that whatever returns the highest regression coefficient is the best predictor. <br><br>\n",
    "While linear regression presents a fine first guess, it poses a few problems. Linear regression assumes atmospheric variables are linearly related to precipitation, AND that atmospheric variables are not collinear (both of which are false assumptions). <br><br>\n",
    "While a linear relationship between predictor & predictand is a good first guess, why limit yourself to linearity when you can just as easily relax that assumption?<br><br>\n",
    "This is where Machine Learning can come in handy. The rest of this notebook will step through the following Machine Learning model pipeline:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_pipeline](./images/model_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.B. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is a huge part of building Machine Learning model \"pipelines\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few steps involved when thinking about building & training a Machine Learning model. Remember, models are \"stupid\", and there are a few statistical \"gotcha's\" that may result in your model being biased, inaccurate, or not suitable for the problem at hand. The order of addressing these issues in your data is important (we will address the importance of this order in question 1 at the end)!\n",
    "<br>\n",
    "1. __What exactly are we trying to predict? A value, an outcome, a category? Define your predictors and predictand.__ This requires revisiting your hypothesis or overarching question. In our case, our predictand is the likelihood of precipitation. We will build a Machine Learning model (actually, a few models) that predicts the likelihood that it's currently precipitating at Christman, CO, given current atmospheric conditions. \n",
    "<br><br>\n",
    "2. __Do you have any missing data? If so, how will you handle them? Keep in mind, decreasing the number of input observations may bias your model.__ In our case, we have no missing data.\n",
    "<br><br>\n",
    "3. __Do you have any categorical or non-numeric variables or features? If so, you must figure out how to encode them into numbers.__ Luckily, in the geosciences, we rarely run into this problem.\n",
    "<br><br>\n",
    "4. __How will we validate our model?__ Typically, people split their existing data into training data and testing data, or perform \"__cross-validation__\" or a \"__test-train split__\". That is, we will \"hold out\" some data and call it our \"testing data\", while using the rest of the data to train our model (i.e., \"training data\"). Once our model is trained, we will evaluate its performance with the holdout testing data. *Note:* This could be problematic if there is limited data. \n",
    "<br><br>\n",
    "5. __Do your features have the same variance?__ Just like for K-means clustering above, this is an important step to ensuring your model doesn't depend on one variable with large variance too heavily. This step is called \"__feature scaling__\". Features of the same size also speed up the Gradient Descent algorithm.\n",
    "<br><br>\n",
    "6. __If classification is the goal, are there the same number of observations for each feature and outcome? If not, how will you rebalance?__ Luckily, we have the same number of observations (8784) for each feature, but a lack of precipitation is way more common than hours with precipitation. Thus, we will simply oversample the observations associated with precip so that the two outcomes (or \"classes\") are equal. *Note: It's important that feature scaling or normalization is performed before any rebalancing so that the qualitative statistics (mean, stddev, etc) remain the same.*\n",
    "<br><br>\n",
    "7. __What are the appropriate metrics for assessing your model? Consider the bias-variance trade-off, and whether having false positives or false negatives is more impactful.__ In our case, predicting no rain when there is rain (false negative) is probably more frustrating and potentially more impactful than the other way around (a false positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What exactly are you trying to predict?\n",
    "First, split data into predictor & predictands. \n",
    "I'm going to create (or \"engineer\") a new feature that indicates whether precipitation occurred. Clearly, this step only needs to be performed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # what is the variable called that indicates precipitation amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prec_occur'] = np.array(df.Prec_inches!=0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, select the data that will be predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.copy(deep=True) \n",
    "# deep = True so that changes to predictors won't be made to df.\n",
    "\n",
    "# these shouldn't go into predicting whether or not there is rain.\n",
    "predictors = df.drop(['day','hour','Prec_inches'],axis=1) \n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that worked. Now I will assign everything but \"prec\" to be the predictor array \"x\", and prec will be the predictand vector \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = predictors.drop('prec_occur',axis=1)\n",
    "y = predictors.prec_occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 & Q3 do not need to be addressed in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How will you validate your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, perform a test-train split to ensure we can validate our trained model. <br>\n",
    "\n",
    "This step must be performed before each time the model is trained to ensure we are not baking in any bias among the models we train. That also means the following two steps must also be performed prior to training each model as well. For this reason, I wrote functions to call easily before each model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_holdout_data(x, y, verbose):\n",
    "    \"\"\"Perform a 80/20 test-train split (80% of data is training, 20% is testing). Split is randomized with each call.\"\"\"\n",
    "    random_state = randint(0,1000)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=random_state)\n",
    "    if verbose==True:\n",
    "        print(\"Prior to scaling and rebalacing...\")\n",
    "        print(\"Shape of training predictors: \"+str(np.shape(x_train)))\n",
    "        print(\"Shape of testing predictors: \"+str(np.shape(x_test)))\n",
    "        print(\"Shape of training predictands: \"+str(np.shape(y_train)))\n",
    "        print(\"Shape of testing predictands: \"+str(np.shape(y_test)))\n",
    "        print(\" \")\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Do your features have the same variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must normalize the features, or perform \"__Feature Scaling__\", for the same reasons why we normalized prior to running the K-Means clustering algorithm. \n",
    "  \n",
    "The difference here is that I will keep the data as a pandas dataframe rather than converting it to a numpy array beforehand. The \"fit_transform\" function outputs a numpy array, but we will convert back to a dataframe so that re-balancing the dataset is easier. \n",
    "  \n",
    "*__Note__*: If my predictand wasn't binary, then I would also want to normalize that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(x_train, x_test):\n",
    "    \"\"\"\n",
    "    Scale training data so that model reaches optimized weights much faster. Scaling could mean \n",
    "    standardizing (so that the standard deviation is 1) or normalizing (the max and min are 1 and -1). \n",
    "    Here we normalize.\n",
    "    \n",
    "    *All data that enters the model should use the same scaling used to scale the training data.*\n",
    "    Thus, we also perform scaling on testing data for validation later. \n",
    "    Additionally, we return the scaler used to scale any other future input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler() # normalize \n",
    "    x_train_scaled = pd.DataFrame(data=scaler.fit_transform(x_train),index=x_train.index,columns=x_train.columns) \n",
    "    x_test_scaled = pd.DataFrame(data=scaler.transform(x_test),index=x_test.index,columns=x_test.columns)\n",
    "    \n",
    "    return scaler, x_train_scaled, x_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Are there the same number of observations for each outcome or class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we have the same number of observations for each feature (8784). But do we have the same number of outcomes for our predictand - i.e., the same number of hours that are precipitating as those that are non-precipitating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prec_occur'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not. The outcomes we are trying to predict are extremely unbalanced. Non-precip hours occur 30x more than precip hours. This class imbalance may bias the model because precip hours are underrepresented, which means the model won't have as many instances of precip hours to learn to distinguish precip hours from non-precip hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of out-of-the-box functions that resample data very precisely. The one I use below simply randomly oversamples the existing precipitating observation data to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: This function should be called on both training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x,y,verbose):\n",
    "    \"\"\"Resample data ensure model is not biased towards a particular outcome of precip or no precip.\"\"\"\n",
    "    # Combine again to one dataframe to ensure both the predictor and predictand are resampled from the same \n",
    "    # observations based on predictand outcomes. \n",
    "    dataset = pd.concat([x, y],axis=1)\n",
    "\n",
    "    # Separating classes\n",
    "    raining = dataset[dataset['prec_occur'] == 1]\n",
    "    not_raining = dataset[dataset['prec_occur'] == 0]\n",
    "\n",
    "    random_state = randint(0,1000)\n",
    "    oversample = resample(raining, \n",
    "                           replace=True, \n",
    "                           n_samples=len(not_raining), #set the number of samples to equal the number of the majority class\n",
    "                           random_state=random_state)\n",
    "\n",
    "    # Returning to new training set\n",
    "    oversample_dataset = pd.concat([not_raining, oversample])\n",
    "\n",
    "    # reseparate oversampled data into X and y sets\n",
    "    x_bal = oversample_dataset.drop(['prec_occur'], axis=1)\n",
    "    y_bal = oversample_dataset['prec_occur']\n",
    "\n",
    "    if verbose==True:\n",
    "        print(\"After scaling and rebalacing...\")\n",
    "        print(\"Shape of predictors: \"+str(np.shape(x_bal)))\n",
    "        print(\"Shape of predictands: \"+str(np.shape(y_bal)))\n",
    "        print(\" \")\n",
    "    \n",
    "    return x_bal, y_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's put the data prep code from questions 1-6 into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_pipeline(x, y, verbose):\n",
    "    \"\"\" Combines all the functions defined above so that the user only has to \n",
    "    call one function to do all data pre-processing. \"\"\"\n",
    "    # verbose=True prints the shapes of input & output data\n",
    "\n",
    "    # split into training & testing data\n",
    "    x_train, x_test, y_train, y_test = define_holdout_data(x, y, verbose) \n",
    "\n",
    "    # perform feature scaling\n",
    "    scaler, x_train_scaled, x_test_scaled = scale_data(x_train, x_test)\n",
    "\n",
    "    # rebalance according to outcomes (i.e., the number of precipitating \n",
    "    # observations & non-precipitating outcomes should be equal)\n",
    "    if verbose==True:\n",
    "        print(\"for training data... \")\n",
    "    x_train_bal, y_train_bal = balance_data(x_train_scaled, y_train, verbose)\n",
    "    if verbose==True:\n",
    "        print(\"for testing data... \")\n",
    "    x_test_bal, y_test_bal = balance_data(x_test_scaled, y_test, verbose)\n",
    "    \n",
    "    return x_train_bal, y_train_bal, x_test_bal, y_test_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. What are the appropriate metrics for assessing your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics will be used to evaluate the model after training. Thus, these functions will also be called for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, define functions for evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm simply going to train various models and then look at their model metrics. Then we can make a prediction based on some selected observation to obtain a likelihood that it's raining, and we can compare with our own eyes whether the model gets it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some commonly-used metrics for assessing the value of a given Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**True Positive (TP)**\" Is the number of times the model predicts a positive when the observation is actually positive. In our case, the model predicts that its raining when it is actually raining.<br>\n",
    "\"**False Positive (FP)**\" The number of times the model guesses that it's raining when it's not actually raining.<br>\n",
    "The same applies to **True Negatives (TN)** (correctly predicting that it's not raining) and **False Negatives (FN)** (predicting no rain when it's actually raining).\n",
    "\n",
    "\n",
    " - **Precision = TP/(TP + FP)**: The proportion of predicted precipitating events that are actually precipitating.\n",
    " - **Accuracy = (TP + TN)/(total)**: The proportion of precipitating hours or non-precipitating hours that are correctly predicted by the model.\n",
    " - **Recall = TP/(TP + FN)**: The proportion of precipitating hours that are correctly predicted by the model.<br>\n",
    "<br>\n",
    "Other important metrics that we aren't going to look at today:\n",
    " - **F1**: a way to capture how well the model predicts the hours that it's actually precipitating.\n",
    " - **ROC/AUC**: how well the model separates precipitating hours from non-precipitating hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I define a couple of functions that will help us visualize the metrics listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rounded metrics for each model.\n",
    "def bin_metrics(x, y):\n",
    "    \"\"\"Prints accuracy and recall metrics for evaluating \n",
    "    classification predictions.\"\"\"\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(x, y)\n",
    "    recall = metrics.recall_score(x, y)\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 4))\n",
    "    print('Recall:', round(recall, 4))\n",
    "    \n",
    "    return accuracy, recall\n",
    "\n",
    "\n",
    "# Pplot confusion matrix\n",
    "def plot_cm(x, y):\n",
    "    \"\"\"Plots the confusion matrix to visualize true \n",
    "    & false positives & negatives\"\"\"\n",
    "    cm = confusion_matrix(x, y)\n",
    "    df_cm = pd.DataFrame(cm, columns=np.unique(x), index = np.unique(x))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 25}, fmt='g')# font size\n",
    "    plt.ylim([0, 2])\n",
    "    plt.xticks([0.5, 1.5], ['Negatives','Positives'])\n",
    "    plt.yticks([0.5, 1.5], ['Negatives','Positives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our answer to number 7 under the \"1. Data Preparation\" section above, I will be using **Accuracy** and **Recall** to compare these four different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way we can evaluate the models is to compare precipitation likelihood given the same set of atmospheric conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's choose some observation in the pre-scaled dataset shows that it's raining, and then find the corresponding scaled observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_atmos_conditions_precip(index='rand'):\n",
    "    \"\"\"\n",
    "    Function returns atmospheric conditions in a dataframe as well as the scaled\n",
    "    conditions in a numpy array so that they output a prediction in the model.\n",
    "    \n",
    "    If no input is passed, the function will randomly generate an in index to \n",
    "    choose from those observations in some training data with precipitation. \n",
    "    Otherwise, an integer index between 0 and 200 should be passed.\n",
    "    \"\"\"\n",
    "    # First, perform a test-train split\n",
    "    x_train, x_test, y_train, _ = define_holdout_data(x, y, verbose=False) \n",
    "\n",
    "    # perform feature scaling - this happens *AFTER* the test-train split\n",
    "    _, x_train_scaled, _ = scale_data(x_train, x_test)\n",
    "\n",
    "    # this is what will go into the model to output a prediction\n",
    "    if index=='rand':\n",
    "        index = randint(0,len(y_train[y_train==1].index)) \n",
    "    precipindex = y_train[y_train==1].index.values[index]\n",
    "    testpredictor = x_train_scaled.loc[precipindex] \n",
    "    \n",
    "    return df.iloc[precipindex], testpredictor    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will choose some atmospheric conditions after training the first model, and then use those conditions for the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.B Train & compare four ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section below goes through building and training a ML model. In each section, there are a few steps for each model \"pipeline\":\n",
    "1. __Randomly perform a test-train split, feature scaling, and resample data to ensure outcomes are balanced__. \n",
    "2. __Train your model__.\n",
    "3. __Assess model metrics with testing and training data__. We begin by first assessing each model's performance by calculating the metrics defined above on the *testing* or *holdout* data; the key here is that the model has never seen this data. <br>__If applicable, tune your model.__ This means choosing new *hyperparameters*, retraining the model, and then reassessing the same model metrics to see if the model yields better results.\n",
    "3. __Check for model overfitting__. We will also check to see if the model is overfitting by comparing metrics of the testing data to that of the training data. In short, the training data should not be outperforming the testing data.\n",
    "4. __Actually make a prediction with a single observation__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.B.a. [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just start with a simple Logistic Regression. A Logistic Regression is a very specialized model in that in that it only computes the probability of a binary outcome.  \n",
    "![logistic regression gif](images/logistic_regression.gif)\n",
    "<br>\n",
    "This model also requires that the assumptions of linear regression must hold, i.e., <br>\n",
    "1. linear relationship between predictors & predictand, \n",
    "2. normally distributed residuals between observed predictand and predicted outcome, \n",
    "3. no collinearity must exist among predictors, i.e., must be independent<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression shouldn't perform any better than a linear regression, so let's just start with this as an illustration of how well linearity assumptions hold in this dataset.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform a test-train split, perform feature scaling, and the rebalance our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "lr = LogisticRegression(solver='lbfgs') \n",
    "# we choose this particular solver because we're not regularizing or penalizing certain features\n",
    "\n",
    "# fit the model to scaled & balanced training data. Side note: this is where *Gradient Descent* occurs.\n",
    "lr.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Assess Logistic Regression's performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've \"trained\" our model, we make predictions using data that the model has never seen before (i.e., our holdout testing data) to see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test_bal)\n",
    "\n",
    "# Call functions defined above to calculate metrics & plot a confusion matrix based on\n",
    "# how well model simulates testing data\n",
    "plot_cm(y_test_bal, y_pred);\n",
    "lr_acc, lr_rec = bin_metrics(y_test_bal, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determined earlier that False Positives are less harmful than False Negatives. Thus, along with accuracy, which essentially measures how close the model is to the target, we should try to maximize recall. Luckily, recall is the highest-scoring metric for this model (recall = ~87%).<br><br>\n",
    "Let's focus on accuracy, though, for a moment. Accuracy tells is the percent of correct predictions, whether precipitating or not. The Logistic Regression model, without any additional tuning, can correctly predict whether it's precipitating or not given a set of present atmospheric conditions around 84% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check to see if the Logistic Regression model is overfitting (or underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important aspect of tuning machine learning model is to ensure the model isn't overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overfitting.v.underfitting](images/underfitting_overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An underfit model is a biased model. Symptoms of underfitting are that your training data performs worse than your testing data, or the model metrics are just poor overall. To alleviate underfitting, introduce more variance into the model by adding more features, use ensemble methods, or increase model complexity. <br><br>\n",
    "An overfit model means the model is fit very well to the training data, but fails to generalize predictions outside the training dataset. A symptom of overfitting is that the models' training accuracy is much better than the testing accuracy. Overfitting can happen more easily in more complex models, like neural networks. To alleviate overfitting, one needs to reduce variance, through **feature regularization**, lowering model complexity, or performing k-folds cross-validation.<br><br>\n",
    "Before you dive too deeply into ML and in your own time, I suggest watching [this 6-minute StatQuest YouTube video](https://www.youtube.com/watch?v=EuBBz3bI-aA) to develop an intuition of error, overfitting, and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "pred_train= lr.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:<br>\n",
    "testing metrics > training metrics = underfitting, model is too simple<br>\n",
    "testing metrics < training metrics = overfitting, model is too complex<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Make a prediction with the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we randomly choose some atmospheric conditions using the function defined above. This will be the atmospheric conditions we use for all models we build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origvals, testpredictor = rand_atmos_conditions_precip()\n",
    "# print(origvals) # observation from original dataframe\n",
    "# print(testpredictor) # scaled observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "lr_prediction = lr.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(\"There is a {0:.{digits}f}% chance of precipitation given those meteorological conditions.\".format(lr_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.B.b. [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand random forests, one must first understand a [decision tree](https://scikit-learn.org/stable/modules/tree.html#tree). A decision tree is intuitive: it is essentially a flowchart to point to an outcome based on \"decisions\" for each feature. <br><br>\n",
    "A Random Forest is an ensemble of decision trees that are randomly constructed based on the features of the dataset and number of decisions. Trees are constructed by randomly choosing a feature to \"seed\" each tree, and then making rules or associations with other features to lead to the specified outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random_forest](images/Random_forest_diagram_complete.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform a test-train split, perform feature scaling, and the rebalance our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a train-test split for cross-validation, perform feature scaling, and rebalance each testing & training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train (and tuning) the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, in Machine Learning models, we can choose a hyperparameters that control the behavior of the model and can improve model performance. These hyperparamters are just numbers that tell the model to act in different ways. Adjusting hyperparameters can control the complexity of the model, and thus, influence model metrics. <br><br>\n",
    "There are many hyperparameters one can decide upon when tuning the Random Forest classifier (see scikit-learn page on Random Forests for more examples). The two we will be adjusting is \n",
    "1. The number of estimators or \"trees\" in the forest\n",
    "2. The depth of the tree, or how many \"decisions\" are made until convergence is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "num_est = [10, 50, 500] # number of trees\n",
    "depth = [2, 10, 100] # number of decisions\n",
    "for i in num_est:\n",
    "    start = time.time()\n",
    "    print(\"Number of estimators is \"+str(i))\n",
    "\n",
    "    for k in depth:\n",
    "        print(\"depth is \"+str(k))\n",
    "        forest = RandomForestClassifier(n_estimators=i, max_depth=k)\n",
    "        forest.fit(x_train_bal, y_train_bal)\n",
    "        \n",
    "        # cross validate & evaluate metrics based on testing data\n",
    "        pred_test= forest.predict(x_test_bal)\n",
    "        acc_val = metrics.accuracy_score(y_test_bal, pred_test)\n",
    "        acc_scores.append(acc_val)\n",
    "        rec_val = metrics.recall_score(y_test_bal, pred_test)\n",
    "        rec_scores.append(rec_val)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Random Forest took \"+str(end-start)+\" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_scores, marker='o', color='black')\n",
    "plt.plot(rec_scores, marker='o', color='blue')\n",
    "print(\"Max Accuracy (black):\", round(max(acc_scores), 4))\n",
    "print(\"Max Recall (blue):\", round(max(rec_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x =** <br>\n",
    "0, 1, 2: number of estimators = 10<br>\n",
    "3, 4, 5: number of estimators = 50<br>\n",
    "6, 7, 8: number of estimators = 500<br>\n",
    "<br>\n",
    "0, 3, 6: depth range = 2<br>\n",
    "1, 4, 7: depth range = 10<br>\n",
    "2, 5, 8: depth range = 100<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right hyperparameters for this model requires revisiting which metrics are most important to our question. We want to maximize both recall and accuracy. <br><br>\n",
    "For simplicity, I choose the parameters corresponding to x=0, but __I highly suggest you try other combinations of parameters as well__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, max_depth=2);\n",
    "forest.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Assess the Random Forest's performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will use our testing data to make an initial evaluation of how the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test= forest.predict(x_test_bal)\n",
    "\n",
    "# Call functions defined above to calculate metrics & plot a confusion matrix based on\n",
    "# how well model simulates testing data\n",
    "forest_acc, forest_rec = bin_metrics(y_test_bal, pred_test)\n",
    "plot_cm(y_test_bal, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check to see if the Random Forest is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "rf_pred_train= forest.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,rf_pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:<br>\n",
    "testing metrics > training metrics = underfitting, model is too simple<br>\n",
    "testing metrics < training metrics = overfitting, model is too complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests seldom overfit, but if they do, one should try increasing the number of trees, or decreasing the amount of data used to construct each tree. See scikit-learn's [Random Forest Classifier webpage](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for information on more hyperparameters one can tune to address overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Make a prediction with the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "forest_prediction = forest.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(\"There is a {0:.{digits}f}% chance of precipitation given those meteorological conditions.\".format(forest_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The random forest yields yet another percent chance of precipitation given the meteorological conditions... hmm...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.B.c. [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "SVMs divide observations into classes based on maximizing the distance between a \"kernel\" (basically a dividing function) and the elements of each feature/class/variable on a plane.<br>\n",
    "![SVM image](images/sphx_glr_plot_iris_svc_0011.png)<br>*A diagram illustrating classification with various SVM kernels using scikit-learn's iris dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the relationships between atmospheric variables and precipitation are inherently non-linear, we will choose a non-linear, \"RBF\" kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform a test-train split, perform feature scaling, and the rebalance our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat a different random train-test split for cross-validation, perform feature scaling, and rebalance each testing & training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train (and tuning) the SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of SVMs, we can tune \"C\", the regularization parameter. [Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) penalizes higher-order coefficients during training (i.e., Gradient Descent). Regularization is a way to reduce a model's complexity and address overfitting. <br><br>In SVMs, the lower the regularization parameter C, the higher the penalty. We are unsure what the C value should be. Thus, we train the model three times, each with a different value of C to see what the best value should be. *I highly suggest learning more on [regularization](https://www.youtube.com/watch?v=Q81RR3yKn30) if you choose to pursue ML methods on your own.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Note: This cell takes about ~1 minute to run__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "C_range = [0.01, 1, 100]\n",
    "for i in C_range:\n",
    "    start = time.time()\n",
    "    print(\"C is... \"+str(i))\n",
    "    svmclassifier = svm.SVC(C=i, kernel='rbf', gamma='scale', max_iter=20000, probability=True)\n",
    "    svmclassifier.fit(x_train_bal, y_train_bal)\n",
    "    \n",
    "    # Save model metrics in order to choose best hyperparameter\n",
    "    pred_test= svmclassifier.predict(x_test_bal)\n",
    "    acc_val = metrics.accuracy_score(y_test_bal, pred_test)\n",
    "    acc_scores.append(acc_val)\n",
    "    rec_val = metrics.recall_score(y_test_bal, pred_test)\n",
    "    rec_scores.append(rec_val)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Took \"+str(end-start)+\" seconds to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_scores, marker='o', color='black')\n",
    "plt.plot(rec_scores, marker='o', color='blue')\n",
    "\n",
    "print(\"Max Accuracy (black):\", round(max(acc_scores), 4))\n",
    "print(\"Max Recall (blue):\", round(max(rec_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM with C=1, i.e., a medium weight penalty, results in a balance among accuracy, precision, and recall. <br>\n",
    "We will train our final model with this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SVM classifier & fit to training data\n",
    "svmclassifier = svm.SVC(C=1, kernel='rbf', gamma='scale', max_iter=20000, probability=True)\n",
    "svmclassifier.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Assess SVM performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we throw some data that the model has never seen before to assess how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test= svmclassifier.predict(x_test_bal)\n",
    "\n",
    "# Call functions defined above to calculate metrics & plot a confusion matrix based on\n",
    "# how well model simulates testing data\n",
    "svm_acc, svm_rec = bin_metrics(y_test_bal, pred_test)\n",
    "plot_cm(y_test_bal, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing to our last model, using a non-linear Singular Vector Machine instead of a Logistic Regressor increased the recall and accuracy. Looking better... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check to see if the SVM is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "svm_pred_train= svmclassifier.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,svm_pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:<br>\n",
    "testing metrics > training metrics = underfitting, model is too simple<br>\n",
    "testing metrics < training metrics = overfitting, model is too complex<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can address overfitting in an SVM by changing the kernel to a simpler kernel, or tuning the regularization parameter C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Make a prediction with the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "svm_prediction = svmclassifier.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(\"There is a {0:.{digits}f}% chance of precipitation given those meteorological conditions.\".format(svm_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How does the SVM compare to the linear regression model in terms of assessing precipitation likelihood for the given meteorological observation?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.B.d. [Neural Network](https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1)\n",
    "![neural net chart](images/neural_net.gif)<br>\n",
    "![neural net equation](images/neural_net_equation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: there is a TON of information online about Neural Networks.* I suggest starting with the 3-part blog series on neural networks accessible in the link above for a gentle introduction to the theory. <br><br>\n",
    "If you'd rather absorb new information through watching a video, I found [this three-part series of youtube videos](https://www.youtube.com/watch?v=aircAruvnKk) (totaling about an hour in length) to be very helpful.<br><br>\n",
    "Another great resource is [machinelearningmastery.com](machinelearningmastery.com). In fact, the model below is based off of [this blog post](https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform a test-train split, perform feature scaling, and the rebalance our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test & training data, perform feature scaling, and rebalance dataset according to outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train (and build and compile) the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of hyperparameters here. Please read the comments to guide you in playing with them later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a very simple Neural Network & compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_inputs = len(x_train_bal.columns)\n",
    "\n",
    "# create model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(number_inputs, input_dim=number_inputs, activation='relu'))\n",
    "\n",
    "# Try uncommenting this to address overfitting\n",
    "# from keras.regularizers import l2\n",
    "# reg = l2(0.001)\n",
    "# nn.add(Dense(number_inputs, activation='relu',bias_regularizer=reg,activity_regularizer=reg))\n",
    "\n",
    "# try commenting out one and then the other\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "#nn.addDense(1, activation='softmax'))\n",
    "\n",
    "# Compile model \n",
    "# Also try changing the learning rate.\n",
    "learning_rate = 0.001 # only used in the SGD optimizer.\n",
    "\n",
    "# Also try commenting out one & then the other. \n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#nn.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=learning_rate), metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 112497,
     "status": "ok",
     "timestamp": 1587745942845,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhxGyQiqej0v5Oem2a-mzfI3Kg0wR7mkUvEhP32K4w=s64",
      "userId": "08898050549780290733"
     },
     "user_tz": 360
    },
    "id": "B9KJsrjltNTs",
    "outputId": "c93f4b23-728c-4270-f826-e6125555aacf"
   },
   "outputs": [],
   "source": [
    "batch_size = 24 # The number of samples the network sees before it backpropagates (batch size) # 24 & 32 yield accuracy = 87%\n",
    "epochs = 100 # The number of times the network will loop through the entire dataset (epochs)\n",
    "shuffle = True # Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "verbose = 2 # Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "\n",
    "# Train the neural network!\n",
    "start = time.time()\n",
    "\n",
    "history = nn.fit(x_train_bal, y_train_bal, validation_data=(x_test_bal, y_test_bal), \n",
    "          batch_size=batch_size, epochs=epochs, shuffle=shuffle, verbose=verbose)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Neural Network took \"+str(end-start)+\" seconds to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy & loss with epochs\n",
    "Neural networks train in __epochs__. During each epoch, the model trains by sweeping over each layer, adjusting weights based on their resulting errors, through processes called __forward propagation__ and __backpropagation__. By plotting the model accuracy & loss which each epoch, we can visualize how the model error evolves with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows=2,ncols=1)\n",
    "figure.tight_layout(pad=3.0)\n",
    "\n",
    "# plot accuracy during training\n",
    "plt.subplot(211)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend();\n",
    "\n",
    "# plot loss during training\n",
    "plt.subplot(212)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.xlabel(\"Epoch\");\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Assess Neural Network's performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the accuracy is pictured above, additionally quantify recall on testing data with the same functions used previously to remain consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test= (nn.predict(x_test_bal)>0.5).astype(\"int32\")\n",
    "nn_acc, nn_rec = bin_metrics(y_test_bal, pred_test)\n",
    "plot_cm(y_test_bal, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check to see if the Neural Network is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "nn_pred_train= (nn.predict(x_train_bal)>0.5).astype(\"int32\")\n",
    "bin_metrics(y_train_bal,nn_pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can easily overfit because they are very complex and can fit to the training data extremely well,  which prevents them from generalizing to other data (like the testing data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Make a prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "nn_prediction = nn.predict(np.array(testpredictor).reshape(1, -1))[0][0]*100\n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\"There is a {0:.{digits}f}% chance of precipitation given those meteorological conditions.\".format(nn_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interesting... How does the Neural Network's estimate of %chance precip compare to that of the other models? See below for final intermodel comparison.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final intermodel comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = pd.DataFrame({'Metrics':['Accuracy','Recall','Prediction example'],\n",
    "     'Logistic Regression':[lr_acc, lr_rec, lr_prediction],\n",
    "    'Random Forest':[forest_acc, forest_rec, forest_prediction],\n",
    "    'Singular Vector Machine':[svm_acc, svm_rec, svm_prediction],\n",
    "    'Neural Network':[nn_acc, nn_rec, nn_prediction]})\n",
    "model_metrics = model_metrics.set_index('Metrics')\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the way one would identify the most important features in each model, i.e., the best predictors of rainfall. The code looks a little different from model to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to look at the pre-scaled data used in the models, uncomment the line below\n",
    "#x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance in Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(abs(lr.coef_[0]),\n",
    "             index = x.columns,\n",
    "             columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance is not possible with non-linear Singular Vector Machines** because the data is transformed by the kernel into another space that is unrelated to the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding most important features in a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(forest.feature_importances_,\n",
    "                                   index = x.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = x.columns.values\n",
    "nn_featimportance = []\n",
    "for var in cols:\n",
    "    # create a vector corresponding to a 1 where the feature is located:\n",
    "    inputvector = np.array((cols==var).astype(int).reshape(1, -1))\n",
    "    nn_featimportance.append(nn.predict(inputvector)[0][0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( nn_featimportance,\n",
    "             index = x.columns,\n",
    "             columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __One important \"gotcha\" in ML is the order of data preparation. Why should one should perform the train-test split *before* feature scaling and rebalancing? *Hint: think about using a trained model for future predictions. Why perform a test-train split at all? Also, what data is considered when scaling a dataset?*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised.answer1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Why did we choose to use accuracy, recall, and predicted precipitation probability as a way to compare models?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised.answer2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Now that you have some exposure to various models, what do you think might be some pros and cons regarding each model?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised.answer3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Collinearity, or non-zero correlation among features, results in a model that is overly complex, reduces the statistical significance of the fit of the model, and prevents one from correctly identifying the importance of features. Are there features included in these models that are collinear? You may assess this by running the code below. If so, how do you think we should address collinearity?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 15])\n",
    "ax = sns.heatmap(x.corr(), cmap='coolwarm', annot=True, linewidths=0.2, square=True, vmin=-1, vmax=1,annot_kws={\"size\": 18});\n",
    "plt.xticks(fontsize=20,rotation=45);\n",
    "plt.yticks(fontsize=20,rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised.answer4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load collinearity_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. __For fun:__\n",
    "    - __Try a different set of parameters for the random forest. How do the metrics change? Does the model overfit?__\n",
    "    - __Try playing around with the neural network to see what happens. You can change the optimizer, the batch size, the number of epochs, the activation function of the last layer, and the learning rate. How do the metrics change? Does the model overfit? Do you run into errors?__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objanalysis_ML",
   "language": "python",
   "name": "objanalysis_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
